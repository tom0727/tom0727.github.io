<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
  <title>初学爬虫小记 - tom0727&#39;s blog</title>
  <meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">


<meta name="author" content="tom0727" /><meta name="description" content="写博客好累啊，是我太久没有写作了吗 这次来记录一下我第一次学习爬虫的经历 起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とに" /><meta name="keywords" content="tom0727, blog, huzhenwei, 胡振为" />


<meta name="baidu-site-verification" content="tom0727" />
<meta name="google-site-verification" content="tom0727" />


<meta name="generator" content="Hugo 0.69.2 with theme even" />


<link rel="canonical" href="https://tom0727.github.io/post/002-%E7%88%AC%E8%99%AB/" />
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/manifest.json">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<link href="/sass/main.min.2ccc99aa18501eb0a20678a919404779fa1d9f66663cef10606eef97052b4b73.css" rel="stylesheet">



<meta property="og:title" content="初学爬虫小记" />
<meta property="og:description" content="写博客好累啊，是我太久没有写作了吗 这次来记录一下我第一次学习爬虫的经历 起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とに" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://tom0727.github.io/post/002-%E7%88%AC%E8%99%AB/" />
<meta property="article:published_time" content="2021-02-01T22:45:03+08:00" />
<meta property="article:modified_time" content="2021-02-06T11:51:33+08:00" />
<meta itemprop="name" content="初学爬虫小记">
<meta itemprop="description" content="写博客好累啊，是我太久没有写作了吗 这次来记录一下我第一次学习爬虫的经历 起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とに">
<meta itemprop="datePublished" content="2021-02-01T22:45:03&#43;08:00" />
<meta itemprop="dateModified" content="2021-02-06T11:51:33&#43;08:00" />
<meta itemprop="wordCount" content="3150">



<meta itemprop="keywords" content="爬虫,python," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="初学爬虫小记"/>
<meta name="twitter:description" content="写博客好累啊，是我太久没有写作了吗 这次来记录一下我第一次学习爬虫的经历 起因 补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补とに"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->

</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">tom0727&#39;s blog</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <a href="/">
        <li class="mobile-menu-item">Home</li>
      </a><a href="/post/">
        <li class="mobile-menu-item">Post</li>
      </a><a href="/tags/">
        <li class="mobile-menu-item">Tags</li>
      </a><a href="/categories/">
        <li class="mobile-menu-item">Categories</li>
      </a><a href="/about/">
        <li class="mobile-menu-item">About</li>
      </a><a href="/search/">
        <li class="mobile-menu-item">Search</li>
      </a>
  </ul>
</nav>
  <div class="container" id="mobile-panel">
    <header id="header" class="header">
        <div class="logo-wrapper">
  <a href="/" class="logo">tom0727&#39;s blog</a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    <li class="menu-item">
        <a class="menu-item-link" href="/">Home</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/post/">Post</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/tags/">Tags</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/categories/">Categories</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/about/">About</a>
      </li><li class="menu-item">
        <a class="menu-item-link" href="/search/">Search</a>
      </li>
  </ul>
</nav>

    </header>

    <main id="main" class="main">
      <div class="content-wrapper">
        <div id="content" class="content">
          <article class="post">
    
    <header class="post-header">
      <h1 class="post-title">初学爬虫小记</h1>

      <div class="post-meta">
        <span class="post-time" title="2021-02-01 22:45:03 &#43;0800 &#43;0800"> 2021-02-01 </span>
        <div class="post-category">
            <a href="/categories/%E5%B7%A5%E7%A8%8B/"> 工程 </a>
            </div>
        <span id="busuanzi_container_page_pv" class="more-meta"> <span id="busuanzi_value_page_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> times read </span>
      </div>
    </header>

    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#起因">起因</a></li>
        <li><a href="#爬-manga1001com">爬 manga1001.com</a></li>
        <li><a href="#爬-manhuaguicom">爬 manhuagui.com</a>
          <ul>
            <li><a href="#step-1">Step 1</a></li>
            <li><a href="#step-2">Step 2</a></li>
            <li><a href="#step-3">Step 3</a></li>
            <li><a href="#step-4">Step 4</a></li>
            <li><a href="#step-5">Step 5:</a></li>
            <li><a href="#step-6">Step 6:</a></li>
          </ul>
        </li>
        <li><a href="#后记">后记</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>
    <div class="post-content">
      <p><del>写博客好累啊，是我太久没有写作了吗</del></p>
<p>这次来记录一下我第一次学习爬虫的经历</p>
<h2 id="起因">起因</h2>
<p>补完とにかくかわいい的番，感觉真好看啊，漫画也不错，就打算补补<a href="https://manga1001.com/%E3%80%90%E7%AC%AC1%E8%A9%B1%E3%80%91%E3%83%88%E3%83%8B%E3%82%AB%E3%82%AF%E3%82%AB%E3%83%AF%E3%82%A4%E3%82%A4-raw/" target="_blank">とにかくかわいい的生肉漫画</a>，但是这网站广告特别多，还会检测我adblocker，禁用javascript的话漫画就加载不出来了，气死我了，一怒之下决定学习爬虫把漫画爬下来看。</p>
<p>爬虫教程有很多，这里特别推荐一个Jack Cui的教程：</p>
<blockquote>
<p><a href="http://www.soolco.com/post/73836_1_1.html" target="_blank">[资源分享]     Python3 网络爬虫：漫画下载，动态加载、反爬虫这都不叫事</a></p>
</blockquote>
<h2 id="爬-manga1001com">爬 manga1001.com</h2>
<p>这个网站设置的比较粗糙，图片都是静态加载的(F12就能看见图片链接)，根据标签<code>soup.find_all()</code>一下即可。对于这个网站的话，简单说一下爬虫的基本流程吧。</p>
<div class="admonition question">
  <p class="admonition-title">基本流程</p>
<ol>
<li>
<p>观察一下页面的HTML，用F12打开可以看到大致结构，如果要看源代码的话，可以选择：</p>
<p>1.1. <code>res = requests.get(url), print(res.txt)</code>
或</p>
<p>1.2. 在url前加上<code>view-source:</code>，然后用浏览器打开。</p>
</li>
<li>
<p>找到包含图片的tag， 找一下规律，然后用 <code>soup.find_all()</code> 即可。</p>
</li>
<li>
<p>获取所有章节的URL，然后分别去每个URL里抓取。</p>
</li>
</ol>
</div>
<details class="admonition note"><summary class="admonition-title">参考代码</summary>
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup
import os
import random
import time

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

root_folder = '/Users/huzhenwei/Desktop/manga/'
create_dir(root_folder)
USER_AGENTS = [
    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,
    &quot;Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)&quot;,
    &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)&quot;,
    &quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot;,
    &quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0&quot;,
    &quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5&quot;,
    &quot;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6&quot;,
    &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&quot;,
    &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20&quot;,
    &quot;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52&quot;,
]


# get content of one chapter
def get_content(folder, prefix, url):

    res = requests.get(url)
    soup = BeautifulSoup(res.content, 'html.parser')
    items = soup.find_all('figure')

    i = 1

    folder_name = os.path.join(folder, f'Chapter_{prefix:03}/')
    create_dir(folder_name)

    for item in items:

        for child in item.children:
            if i != 1:
                img_url = child.get('data-src')
            else:
                img_url = child.get('src')

            print(img_url)
            headers = random.choice(USER_AGENTS)
            img_html = requests.get(img_url, headers)

            img_name = os.path.join(folder_name, f'{i:02}.jpg')
            with open(img_name, 'wb') as file:
                file.write(img_html.content)
                file.flush()

            i += 1
            time.sleep(random.uniform(0, 3.33))  # sleep random time


# get manga url list
def get_url_list(manga_name, url):
    res = requests.get(url)
    soup = BeautifulSoup(res.content, 'html.parser')

    items = soup.find_all('option')

    for i in range(len(items) - 2, -1, -1):
        if items[i] == items[-1]:  # get manga chapter url list without duplicates
            items = items[i+1:]
            break

    chapter = 1
    folder = os.path.join(root_folder, f'{manga_name}/')

    for item in items:
        manga_url = item.get('value')
        get_content(folder, chapter, manga_url)

        print(manga_url)
        chapter += 1


def main():
    url1 = &quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e3%83%88%e3%83%8b%e3%82%ab%e3%82%af%e3%82%ab%e3%83%af%e3%82%a4%e3%82%a4-raw/&quot;
    name1 = 'Tonikaku_Kawaii'
    get_url_list(name1, url1)

    url2 = &quot;https://manga1001.com/%e3%80%90%e7%ac%ac1%e8%a9%b1%e3%80%91%e5%b9%b2%e7%89%a9%e5%a6%b9%e3%81%86%e3%81%be%e3%82%8b%e3%81%a1%e3%82%83%e3%82%93-raw/&quot;
    name2 = 'Umaru_Chan'
    get_url_list(name2, url2)

main()
</code></pre>
</details>
<blockquote>
<p>注: 这里用的<code>USER_AGENT</code>和<code>sleep()</code>都是为了防止被发现然后封IP</p>
</blockquote>
<h2 id="爬-manhuaguicom">爬 manhuagui.com</h2>
<p>上面那个太没挑战性了，于是我打算再爬一个。</p>
<p>打开<a href="https://www.manhuagui.com/comic/27099/354852.html" target="_blank">漫画网站</a>，
<img src="/images/002/1.png" alt="image">
发现没有图片链接，说明是动态加载的图片(用javascript加载的)，那怎么办呢？</p>
<h3 id="step-1">Step 1</h3>
<p>先在网页里找找链接长啥样，毕竟用浏览器浏览的话，图片总是会被加载出来的，然后就能看到链接了，果然，在chrome的<code>Elements</code>这个tag里，我们翻到了图片链接：</p>
<p><img src="/images/002/6.png" alt="image"></p>
<p>不过直接把链接复制到浏览器里打开的话会403，所以我们先搁置一下。</p>
<h3 id="step-2">Step 2</h3>
<p>我们要获得某一话的所有图片链接，可以从图上看出似乎有一大段像是加密后的字符串，我们打开<a href="https://www.manhuagui.com/comic/27099/354390.html" target="_blank">第一话</a>和<a href="https://www.manhuagui.com/comic/27099/" target="_blank">第二话</a>的HTML，用命令行<code>diff</code>一下以后，会发现差异就刚好出现在这串字符串上：<img src="/images/002/2.JPG" alt="image">我们可以肯定这里面包含了图片链接相关的信息。</p>
<h3 id="step-3">Step 3</h3>
<p>既然找到了加密串，那就要找一个钥匙来解码，看一下网页里内容不多，看起来并没有其他有用信息了，但是还有几个<code>.js</code>文件，一个个打开来看一下，终于在其中一个<a href="https://cf.hamreus.com/scripts/config_5F5A8A8B46A7B711EC3579AFD755010FA8E85725.js" target="_blank">文件</a>里找到了一大堆代码，然后这一大堆里面，有一段看起来又被加密了（有点此地无银三百两啊）：
<img src="/images/002/3.png" alt="image"></p>
<p>把这段代码复制到chrome的console里，发现被自动解码了，得到了一个js函数：
<img src="/images/002/4.png" alt="image"></p>
<p>我们点开这个函数，看一下里面的内容：</p>
<details class="admonition note"><summary class="admonition-title">函数内容</summary>
<pre><code>var LZString=(function(){var f=String.fromCharCode;var keyStrBase64=&quot;ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789+/=&quot;;var baseReverseDic={};function getBaseValue(alphabet,character){if(!baseReverseDic[alphabet]){baseReverseDic[alphabet]={};for(var i=0;i&lt;alphabet.length;i++){baseReverseDic[alphabet][alphabet.charAt(i)]=i}}return baseReverseDic[alphabet][character]}var LZString={decompressFromBase64:function(input){if(input==null)return&quot;&quot;;if(input==&quot;&quot;)return null;return LZString._0(input.length,32,function(index){return getBaseValue(keyStrBase64,input.charAt(index))})},_0:function(length,resetValue,getNextValue){var dictionary=[],next,enlargeIn=4,dictSize=4,numBits=3,entry=&quot;&quot;,result=[],i,w,bits,resb,maxpower,power,c,data={val:getNextValue(0),position:resetValue,index:1};for(i=0;i&lt;3;i+=1){dictionary[i]=i}bits=0;maxpower=Math.pow(2,2);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}switch(next=bits){case 0:bits=0;maxpower=Math.pow(2,8);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}c=f(bits);break;case 1:bits=0;maxpower=Math.pow(2,16);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}c=f(bits);break;case 2:return&quot;&quot;}dictionary[3]=c;w=c;result.push(c);while(true){if(data.index&gt;length){return&quot;&quot;}bits=0;maxpower=Math.pow(2,numBits);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}switch(c=bits){case 0:bits=0;maxpower=Math.pow(2,8);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}dictionary[dictSize++]=f(bits);c=dictSize-1;enlargeIn--;break;case 1:bits=0;maxpower=Math.pow(2,16);power=1;while(power!=maxpower){resb=data.val&amp;data.position;data.position&gt;&gt;=1;if(data.position==0){data.position=resetValue;data.val=getNextValue(data.index++)}bits|=(resb&gt;0?1:0)*power;power&lt;&lt;=1}dictionary[dictSize++]=f(bits);c=dictSize-1;enlargeIn--;break;case 2:return result.join('')}if(enlargeIn==0){enlargeIn=Math.pow(2,numBits);numBits++}if(dictionary[c]){entry=dictionary[c]}else{if(c===dictSize){entry=w+w.charAt(0)}else{return null}}result.push(entry);dictionary[dictSize++]=w+entry.charAt(0);enlargeIn--;w=entry;if(enlargeIn==0){enlargeIn=Math.pow(2,numBits);numBits++}}}};return LZString})();String.prototype.splic=function(f){return LZString.decompressFromBase64(this).split(f)};
</code></pre>
</details>
<p>获得这个函数以后，我们尝试着把之前获得的加密串放进去看看：
<img src="/images/002/5.png" alt="image"></p>
<p>这看起来就正常多了，而且这里面的 <code>04|05|06|...</code> 之类的信息看起来也能和之前找到的图片链接对应上。但是它似乎并没有按照某个特定的规律来，所以可以肯定还有一个函数来处理这个字符串。</p>
<h3 id="step-4">Step 4</h3>
<p>有了这个信息，我们就接着找处理这个字符串的函数，再次观察一下HTML，发现这个Base64的串被包含在了一个<code>&lt;script&gt;</code>当中，长这样：</p>
<details class="admonition note"><summary class="admonition-title">script内容</summary>
<pre><code>&lt;script type=&quot;text/javascript&quot;&gt;window[&quot;\x65\x76\x61\x6c&quot;](function(p,a,c,k,e,d){e=function(c){return(c&lt;a?&quot;&quot;:e(parseInt(c/a)))+((c=c%a)&gt;35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c]);return p;}('X.B({&quot;y&quot;:8,&quot;x&quot;:&quot;w v u t s r&quot;,&quot;q&quot;:&quot;8.1&quot;,&quot;p&quot;:o,&quot;n&quot;:&quot;4&quot;,&quot;l&quot;:[&quot;j.1.2&quot;,&quot;A.1.2&quot;,&quot;h.1.2&quot;,&quot;9-a.1.2&quot;,&quot;b.1.2&quot;,&quot;c.1.2&quot;,&quot;d.1.2&quot;,&quot;f.1.2&quot;,&quot;10.1.2&quot;,&quot;11.1.2&quot;,&quot;12.1.2&quot;,&quot;13.1.2&quot;,&quot;14.1.2&quot;,&quot;15.1.2&quot;,&quot;16.1.2&quot;,&quot;17.1.2&quot;,&quot;g.1.2&quot;,&quot;k.1.2&quot;,&quot;C.1.2&quot;,&quot;P.1.2&quot;,&quot;D.1.2&quot;,&quot;U.1.2&quot;,&quot;W.1.2&quot;,&quot;3.1.2&quot;,&quot;%Y%5%7%6%5%Z%6%7%T.1.2&quot;],&quot;V&quot;:R,&quot;Q&quot;:3,&quot;S&quot;:&quot;/O/z/N/4/&quot;,&quot;M&quot;:0,&quot;L&quot;:&quot;&quot;,&quot;K&quot;:J,&quot;I&quot;:H,&quot;G&quot;:{&quot;e&quot;:F,&quot;m&quot;:&quot;E&quot;}}).i();',62,70,'D4KwDg5sDuCmBGZgCYCsxA03gBgIyD21YADgCFgBRdATlOQHYtLLgsAWZ9LANmducOGZMc/LAGZgYAE6wAkgDsAlgBdmOYDiYAzBQBtYAZwEBjOQEMAtrGCjULUZSzAjCgCbBECo8HMB7H3OAlAAsrJR9vK00dAE9gQG4DQGk5QEYdQHozQD10wA49QHvlQEYndzNLd1cBLGRgBXMIABFTJVMUR2QygE1UcwB9HyMANQBHABVUfU1iHAAvEF6ARU0AYRYp9U4cUWQHOm59HSXOQmQ1KVgANxk3GxZUSm45WAAPJVP3HS6Aa3ajL306pQBXQzGJsMjC9TJpzEolB0dCBkNswPo1PtgHoAppTDp9FYwLUgsBiGQUOJtIp9CE3Mg2ABlACyAAlyNxKAAxASs1lAA==='['\x73\x70\x6c\x69\x63']('\x7c'),0,{})) &lt;/script&gt;
</code></pre>
</details>
<p>看起来这也是一个函数啊，而且这个Base64的串似乎作为参数了，再次动用chrome的console帮助我们解析一下：</p>
<p><img src="/images/002/7.png" alt="image"></p>
<p>这下我们大概可以明白几个事情：</p>
<ol>
<li>这段代码实际上是 <code>window[&quot;eval&quot;](...)</code></li>
<li>省略号部分是一个函数 <code>function(p,a,c,k,e,d)</code>, 以 <code>{}</code> 包起来的是函数内容，那<code>return p;}</code> 后面的想必就是这6个参数。</li>
</ol>
<p>观察一下这6个参数，我们会发现：</p>
<blockquote>
<p><code>p = 'X.B({&quot;y&quot;:8,&quot;x&quot;:&quot;w v u t s r&quot;,&quot;q&quot;:&quot;8.1&quot;,&quot;p&quot;:o,&quot;n&quot;:&quot;4&quot;,&quot;l&quot;:[&quot;j.1.2&quot;,&quot;A.1.2&quot;,&quot;h.1.2&quot;,&quot;9-a.1.2&quot;,&quot;b.1.2&quot;,&quot;c.1.2&quot;,&quot;d.1.2&quot;,&quot;f.1.2&quot;,&quot;10.1.2&quot;,&quot;11.1.2&quot;,&quot;12.1.2&quot;,&quot;13.1.2&quot;,&quot;14.1.2&quot;,&quot;15.1.2&quot;,&quot;16.1.2&quot;,&quot;17.1.2&quot;,&quot;g.1.2&quot;,&quot;k.1.2&quot;,&quot;C.1.2&quot;,&quot;P.1.2&quot;,&quot;D.1.2&quot;,&quot;U.1.2&quot;,&quot;W.1.2&quot;,&quot;3.1.2&quot;,&quot;%Y%5%7%6%5%Z%6%7%T.1.2&quot;],&quot;V&quot;:R,&quot;Q&quot;:3,&quot;S&quot;:&quot;/O/z/N/4/&quot;,&quot;M&quot;:0,&quot;L&quot;:&quot;&quot;,&quot;K&quot;:J,&quot;I&quot;:H,&quot;G&quot;:{&quot;e&quot;:F,&quot;m&quot;:&quot;E&quot;}}).i();'</code></p>
</blockquote>
<blockquote>
<p><code>a = 62, c = 70</code></p>
</blockquote>
<blockquote>
<p><code>k = 'D4KwDg5sDuCmBGZgCYCsxA03gBgIyD21YADgCFgBRdATlOQHYtLLgsAWZ9LANmducOGZMc/LAGZgYAE6wAkgDsAlgBdmOYDiYAzBQBtYAZwEBjOQEMAtrGCjULUZSzAjCgCbBECo8HMB7H3OAlAAsrJR9vK00dAE9gQG4DQGk5QEYdQHozQD10wA49QHvlQEYndzNLd1cBLGRgBXMIABFTJVMUR2QygE1UcwB9HyMANQBHABVUfU1iHAAvEF6ARU0AYRYp9U4cUWQHOm59HSXOQmQ1KVgANxk3GxZUSm45WAAPJVP3HS6Aa3ajL306pQBXQzGJsMjC9TJpzEolB0dCBkNswPo1PtgHoAppTDp9FYwLUgsBiGQUOJtIp9CE3Mg2ABlACyAAlyNxKAAxASs1lAA==='['split']('|')</code></p>
</blockquote>
<blockquote>
<p><code>e = 0, d = {}</code></p>
</blockquote>
<p>唯一需要处理的似乎就是<code>k</code>了，虽然<code>k</code>里没有<code>|</code>这个符号，不过刚才使用<code>LZString.decompressfromBase64()</code>函数解析出来的东西倒是有很多<code>|</code>。</p>
<p>自此真相大白了，我们需要做的事情很简单：</p>
<ol>
<li>提取出<code>p,a,c,k,e,d</code>这6个参数。</li>
<li>将<code>k</code>放进<code>LZString.decompressfromBase64()</code>解析一下。</li>
<li>调用 <code>decode_func</code> （也就是 <code>function(p,a,c,k,e,d)</code> ），得到结果。</li>
</ol>
<p>结果长这样：</p>
<p><img src="/images/002/8.jpg" alt="image"></p>
<p>我们要的图片链接就找到啦！在 <code>files</code> 里。</p>
<h3 id="step-5">Step 5:</h3>
<p>我们还剩下最后一个问题：有了图片链接但是访问不了（403）怎么办？这似乎是一种简单的反爬虫方式，google一下，只要假装我们是从本站（即这个漫画的网站）进去的，而不是从其他地方进去的，就可以访问了。虽然在浏览器上做不到，但是python里可以通过更改<code>Referer</code>的方式来达到：</p>
<pre><code>def get_download_header():
    return {'User-Agent': random.choice(USER_AGENTS), &quot;Referer&quot;: &quot;https://www.manhuagui.com/comic/27099/&quot;}
</code></pre>
<h3 id="step-6">Step 6:</h3>
<p>最后的最后，就是爬虫的基本过程了，不过我们有一段javascript代码需要运行，怎么在python中运行javascript呢？</p>
<ol>
<li>首先保存一下javascript代码，叫 <code>decode_func.js</code>。内容如下：</li>
</ol>
<pre><code>const jsdom = require(&quot;jsdom&quot;);
const { JSDOM } = jsdom;
const dom = new JSDOM(`&lt;!DOCTYPE html&gt;&lt;p&gt;Hello world&lt;/p&gt;`);
window = dom.window;
document = window.document;
XMLHttpRequest = window.XMLHttpRequest;
decode_func = window[&quot;eval&quot;](function(p,a,c,k,e,d){e=function(c){return(c&lt;a?&quot;&quot;:e(parseInt(c/a)))+((c=c%a)&gt;35?String.fromCharCode(c+29):c.toString(36))};if(!''.replace(/^/,String)){while(c--)d[e(c)]=k[c]||e(c);k=[function(e){return d[e]}];e=function(){return'\\w+'};c=1;};while(c--)if(k[c])p=p.replace(new RegExp('\\b'+e(c)+'\\b','g'),k[c]);return p;})
</code></pre>
<ol start="2">
<li>然后用以下这段代码就可以了：</li>
</ol>
<pre><code>def load_js():
    with open(&quot;decode_func.js&quot;, 'r') as file:
        js = file.read()
    context = execjs.compile(js, cwd=&quot;/usr/local/lib/node_modules&quot;)
    return context

context = load_js()
</code></pre>
<p>调用的时候就用
<code>res = context.call((&quot;decode_func&quot;), p,a,c,k,e,d)</code></p>
<p>最终代码如下：</p>
<details class="admonition note"><summary class="admonition-title">参考代码</summary>
<pre><code class="language-python">import requests
from bs4 import BeautifulSoup
import os
import time
import random
import lzstring
import execjs
import re
import json

def create_dir(path):
    if not os.path.exists(path):
        os.makedirs(path)

root_folder = '/Users/huzhenwei/Desktop/manga/'
create_dir(root_folder)

USER_AGENTS = [
    &quot;Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; AcooBrowser; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.0; Acoo Browser; SLCC1; .NET CLR 2.0.50727; Media Center PC 5.0; .NET CLR 3.0.04506)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0; AOL 9.5; AOLBuild 4337.35; Windows NT 5.1; .NET CLR 1.1.4322; .NET CLR 2.0.50727)&quot;,
    &quot;Mozilla/5.0 (Windows; U; MSIE 9.0; Windows NT 9.0; en-US)&quot;,
    &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 2.0.50727; Media Center PC 6.0)&quot;,
    &quot;Mozilla/5.0 (compatible; MSIE 8.0; Windows NT 6.0; Trident/4.0; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; .NET CLR 1.0.3705; .NET CLR 1.1.4322)&quot;,
    &quot;Mozilla/4.0 (compatible; MSIE 7.0b; Windows NT 5.2; .NET CLR 1.1.4322; .NET CLR 2.0.50727; InfoPath.2; .NET CLR 3.0.04506.30)&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/523.15 (KHTML, like Gecko, Safari/419.3) Arora/0.3 (Change: 287 c9dfb30)&quot;,
    &quot;Mozilla/5.0 (X11; U; Linux; en-US) AppleWebKit/527+ (KHTML, like Gecko, Safari/419.3) Arora/0.6&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.8.1.2pre) Gecko/20070215 K-Ninja/2.1.1&quot;,
    &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN; rv:1.9) Gecko/20080705 Firefox/3.0 Kapiko/3.0&quot;,
    &quot;Mozilla/5.0 (X11; Linux i686; U;) Gecko/20070322 Kazehakase/0.4.5&quot;,
    &quot;Mozilla/5.0 (X11; U; Linux i686; en-US; rv:1.9.0.8) Gecko Fedora/1.9.0.8-1.fc10 Kazehakase/0.5.6&quot;,
    &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11&quot;,
    &quot;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_3) AppleWebKit/535.20 (KHTML, like Gecko) Chrome/19.0.1036.7 Safari/535.20&quot;,
    &quot;Opera/9.80 (Macintosh; Intel Mac OS X 10.6.8; U; fr) Presto/2.9.168 Version/11.52&quot;,
]

def get_download_header():
    return {'User-Agent': random.choice(USER_AGENTS), &quot;Referer&quot;: &quot;https://www.manhuagui.com/comic/27099/&quot;}

def load_js():
    with open(&quot;decode_func.js&quot;, 'r') as file:
        js = file.read()
    context = execjs.compile(js, cwd=&quot;/usr/local/lib/node_modules&quot;)
    return context
context = load_js()


def decode(s):
    x = lzstring.LZString()
    decoded_str = x.decompressFromBase64(s)
    return decoded_str.split(&quot;|&quot;)


# p = &quot;&quot;&quot;1h.14({&quot;q&quot;:7,&quot;r&quot;:&quot;s t u v w x&quot;,&quot;y&quot;:&quot;7.1&quot;,&quot;A&quot;:B,&quot;C&quot;:&quot;6&quot;,&quot;D&quot;:[&quot;E.1.2&quot;,&quot;F.1.2&quot;,&quot;G.1.2&quot;,&quot;H.1.2&quot;,&quot;o.1.2&quot;,&quot;I.1.2&quot;,&quot;k.1.2&quot;,&quot;h.1.2&quot;,&quot;d.1.2&quot;,&quot;c.1.2&quot;,&quot;a.1.2&quot;,&quot;9.1.2&quot;,&quot;8.1.2&quot;,&quot;l.1.2&quot;,&quot;b.1.2&quot;,&quot;f.1.2&quot;,&quot;g.1.2&quot;,&quot;i.1.2&quot;,&quot;j.1.2&quot;,&quot;p.1.2&quot;,&quot;J.1.2&quot;,&quot;Y.1.2&quot;,&quot;L.1.2&quot;,&quot;13.1.2&quot;,&quot;K.1.2&quot;,&quot;15.1.2&quot;,&quot;16.1.2&quot;,&quot;17.1.2&quot;,&quot;18.1.2&quot;,&quot;1a.1.2&quot;,&quot;1g.1.2&quot;,&quot;1b.1.2&quot;,&quot;1c.1.2&quot;,&quot;1d.1.2&quot;,&quot;%1e%5%3%4%5%1f%4%3%12.1.2&quot;],&quot;19&quot;:10,&quot;Z&quot;:11,&quot;X&quot;:&quot;/W/z/V/6/&quot;,&quot;U&quot;:0,&quot;T&quot;:&quot;&quot;,&quot;S&quot;:R,&quot;Q&quot;:P,&quot;O&quot;:{&quot;e&quot;:N,&quot;m&quot;:&quot;M&quot;}}).n();&quot;&quot;&quot;
# a,c = 62,80
# k=['', 'jpg', 'webp', '9B', 'E5', '8B', '第02回', '27099', 'P0056', 'P0055', 'P0054', 'P0058', 'P0053', 'P0052', '', 'P0059', 'P0060', 'P0051', 'P0061', 'P0062', 'P0050', 'P0057', '', 'preInit', 'P0048', 'P0063', 'bid', 'bname', '总之就是非常可爱', 'fly', 'me', 'to', 'the', 'moon', 'bpic', '', 'cid', '354852', 'cname', 'files', 'P0044', 'P0045', 'P0046', 'P0047', 'P0049', 'P0064', 'P0068', 'P0066', 'GYeIdl7ujUrxJ1ls7JvwpQ', '1612951385', 'sl', '354596', 'prevId', '356912', 'nextId', 'block_cc', 'status', 'zzjsfckafmttm_lj2l', 'ps1', 'path', 'P0065', 'len', 'false', '35', 'BE', 'P0067', 'imgData', 'P0069', 'P0070', 'P0071', 'P0072', 'finished', 'P0073', 'P0075', 'P0076', 'P0077', 'E6', '9F', 'P0074', 'SMH']
# e = 0
# d = dict()
# res = context.call((&quot;decode_func&quot;), p,a,c,k,e,d)
# print(type(res))
# print(res)


# get content of one chapter
def get_content(title, url):

    create_dir(os.path.join(root_folder, title))
    res = requests.get(f&quot;https://manhuagui.com{url}&quot;, random.choice(USER_AGENTS))
    # print(res.text)
    soup = BeautifulSoup(res.content, 'html.parser')
    items = soup.find_all(lambda tag:tag.name=='script', recursive=True)

    for item in items:
        txt = item.string  # 必须是item.string, 不能是item.txt
        if txt and &quot;return p;&quot; in txt:  # 如果tag里没有文字，txt==None

            parts = txt.split(&quot;return p;}(&quot;)
            part = parts[1][:-2]

            split_res = re.split(r',([0-9]+,[0-9]+,)', part)
            p = split_res[0][1:-1]
            split_res[1] = split_res[1][:-1]
            a, c = map(int, split_res[1].split(','))
            k = split_res[2].split(&quot;'['&quot;)[0][1:]
            k = decode(k)
            e = 0
            d = dict()

            res = context.call((&quot;decode_func&quot;), p,a,c,k,e,d)

            s = re.search('({.+})', res).group(0)  # 找到一个由 {} 包裹的group
            info_dict = json.loads(s)

            files_list = info_dict[&quot;files&quot;]
            path_prefix = 'https://i.hamreus.com' + info_dict[&quot;path&quot;]

            i = 1
            for file_name in files_list:
                complete_path = path_prefix + file_name[:-5]
                print(complete_path)

                res = requests.get(complete_path, headers=get_download_header())
                img_name = os.path.join(root_folder, title, f'{i}.jpg')
                with open(img_name, 'wb') as file:
                    file.write(res.content)
                    file.flush()
                time.sleep(random.uniform(5.0, 10.0))
                i += 1


# get manga url list
def get_url_list(url):
    res = requests.get(url, random.choice(USER_AGENTS))
    soup = BeautifulSoup(res.content, 'html.parser')

    items = soup.find_all('div', {&quot;id&quot;: &quot;chapter-list-1&quot;})

    for manga_list in items:
        links = manga_list.find_all(&quot;a&quot;, recursive=True)
        links = links[1:]

        for link in links:
            title = link.get(&quot;title&quot;)
            ref = link.get(&quot;href&quot;)

            if title[-1] == '卷':
                continue

            print(f&quot;{title}, {ref}&quot;)
            get_content(title, ref)


url = &quot;https://www.manhuagui.com/comic/27099/&quot;
get_url_list(url)

</code></pre>
</details>
<blockquote>
<p>注：上面这块代码被识别成lua语言了，样式出了点问题，可以在markdown里面指定语言，在第一个 <code>```</code> 后面加上语言名即可，如 <code>```python</code></p>
</blockquote>
<h2 id="后记">后记</h2>
<p>写这篇blog比玩爬虫本身还累啊，看来我果然不适合写作文（虽然我从小就深刻的明白这个道理）。不过这篇博客很大程度上也是写给自己看的，作为下次爬虫的参考（不知道下次爬虫要等到什么时候了）。</p>
<p>以后可能会补点儿算法笔记，或者题解之类的。</p>

    </div>

    <div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">tom0727</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
        <span title="2021-02-06 11:51:33 &#43;0800 &#43;0800">2021-02-06</span>
        <a href="https://github.com/tom0727/hugo-blog/commit/1a8ea0b6baf12b6318ed2b15a3925bf256591fe7" title="Init 003-crt.md" target="_blank">(1a8ea0b)</a>
        ，<a href="https://github.com/tom0727/hugo-blog/commits/master/content/post/002-%e7%88%ac%e8%99%ab.md" target="_blank">更新历史</a>
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content"><a rel="license noopener" href="https://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a></span>
  </p>
</div>
<footer class="post-footer">
      <div class="post-tags">
          <a href="/tags/%E7%88%AC%E8%99%AB/">爬虫</a>
          <a href="/tags/python/">python</a>
          </div>
      <nav class="post-nav">
        <a class="prev" href="/post/003-crt/">
            <i class="iconfont icon-left"></i>
            <span class="prev-text nav-default">中国剩余定理介绍</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        <a class="next" href="/post/001-hugo-tutorial/">
            <span class="next-text nav-default">Hugo博客搭建小记</span>
            <span class="next-text nav-mobile">Next</span>
            <i class="iconfont icon-right"></i>
          </a>
      </nav>
    </footer>
  </article>
        </div>
        

  

  
    <script src="https://utteranc.es/client.js"
            repo="tom0727/blog-comments"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://github.com/utterance">comments powered by utterances.</a></noscript>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="social-links">
      <a href="mailto:huzhenweitom@gmail.com" class="iconfont icon-email" title="email" target="_blank"></a>
      <a href="https://github.com/tom0727" class="iconfont icon-github" title="github" target="_blank"></a>
  <a href="https://tom0727.github.io/index.xml" type="application/rss+xml" class="iconfont icon-rss" title="rss"></a>
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io" target="_blank">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - 
    <a class="theme-link" href="https://github.com/ouuan/hugo-theme-even" target="_blank">Even</a>
  </span>

  <div class="busuanzi-footer">
    <span id="busuanzi_container_site_pv"> site pv: <span id="busuanzi_value_site_pv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
      <span class="division">|</span>
    <span id="busuanzi_container_site_uv"> site uv: <span id="busuanzi_value_site_uv"><img src="/img/spinner.svg" alt="spinner.svg"/></span> </span>
  </div>

  <span class="copyright-year">
    &copy; 
    2021
    <span class="heart">
      <a class="iconfont icon-github" title="source code" href="https://github.com/tom0727/hugo-blog" target="_blank"></a>
    </span> 
    <span class="author">tom0727</span>
  </span>
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont icon-up"></i>
    </div>
  </div>
  <script src="/lib/highlight/highlight.pack.js?v=20171001"></script>
  <link href="https://cdn.bootcss.com/highlight.js/9.15.10/styles/tomorrow.min.css" rel="stylesheet">
  <script>hljs.configure({tabReplace: '    '});</script>
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.2.1/dist/jquery.min.js" integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/slideout@1.0.1/dist/slideout.min.js" integrity="sha256-t+zJ/g8/KXIJMjSVQdnibt4dlaDxc9zXr/9oNPeWqdg=" crossorigin="anonymous"></script>
  



<script type="text/javascript" src="/js/main.min.262f18179edf1028d9e6a490b56fbf1be8ccae1bda189f07a3ca83925302f4b8.js"></script>
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        }
    };
  </script>
  <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js" integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin="anonymous"></script>


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-tom0727', 'auto');
	ga('set', 'anonymizeIp', true);
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script id="baidu_push">
  (function(){
    if (window.location.hostname === 'localhost') return;
    var bp = document.createElement('script'); bp.async = true;
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
      bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
    }
    else {
      bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
  })();
</script>




</body>
</html>
